---
title: "Reproducible Research: Analysis of Most Damaging Natural Disasters"
output: 
  html_document:
    keep_md: true
---

## Synopsis/Abstract
In this project, I analyzed NOAA's "Storm Data" database, as an assignment for
Reproducible Research, the fifth course in the John's Hopkins Data Science
Specialization on Coursera. The goal of the analysis was to determine which
events in the database were the most damaging, both in human terms (fatalities
and injuries) and economic terms (property damage). The analysis was performed
using the following R code.  

I found that the events that had caused the greatest
human damage in total were tornadoes, both in terms of fatalities (5661) and
injuries (91407). However, the deadliest events on an individual basis were 
marine accidents/mishaps (2.67 fatalities/event), and the most injurious were
tsunamis (6.45 injuries/event).  

In property damage terms, floods did the greatest total damage ($168212215589),
and hurricanes/typhoons were the most damaging individually ($381055402/event).

In both sections, after answering the initial questions, I explored the
distributions of the totals and averages calculated. Most distributions showed
that most types of events are largely harmless, with a few having extremely
high values in a pattern resembling a power law (unconfirmed). However, the
average economic damage of events seemed to drop off extremely sharply, and
when plotted on a semi-log histogram, showed an interesting clustering rather
than the flat distribution characteristic of a power law.

## Preparing the data

```{r message=F}
library(dplyr) #show code, but hide messages for including packages
```

### Loading in raw data
First, check if a file called data.csv.bz2 is already in the local directory. If
not, download the data from the URL given in the assignment, using the file name
data.csv.bz2. Once the data file is in the local directory, read into R.

```{r cache=TRUE}
if (!file.exists("data.csv.bz2")) {
  download.file("https://d396qusza40orc.cloudfront.net/repdata%2Fdata%2FStormData.csv.bz2", "data.csv.bz2")
}
raw <- read.csv("data.csv.bz2")
```

### Inspecting the data

Next, inspect the data to determine if it needs cleaning and/or pre-processing.

```{r}
dim(raw) #how many rows and columns?
names(raw) #are the columns named, and if so what are they?
sapply(raw, FUN=function(x) sum(is.na(x))) #how many missing values and where?
length(levels(factor(raw$EVTYPE))) #how many event types are there?
head(levels(factor(raw$EVTYPE))) #what are the types?
```

Data size looks sufficient (902297 rows), columns are tidy and meaningfully
labeled, some data is missing but in relatively unimportant columns. However,
the classifications of the events themselves seems like it needs pre-processing,
since two separate kinds of floods and two separate kinds of thunderstorm winds
are listed separately.

### Pre-processing the data

Create new categories using keywords, and ensure that all (meaningful) rows are
included at least once. Rows were regarded as not meaningful if they:  
1. don't describe an event ("Summary of May 13", "Apache County")  
2. are highly ambiguous ("Excessive", "Record temperature")  
3. don't make any sense whatsoever ("Remnants of Floyd", "Southeast")  

Categories were created by adding to the list of strings to search for until all 
un-included types were not meaningful observations. Some researchers' discretion
is unavoidable with this method, but the code below is a record of exactly how
event types were grouped.

Do all of this in a copy of the data, not the original.

```{r cache=TRUE}
data <- raw
data$included = FALSE #create new variable to keep track of inclusion
data$newType = character(length=nrow(data)) #create new variable to track new type membership
types <- c("avalanche|avalance", "blizzard", "coastal storm|coastalstorm", "cold|low temp|record low", "cool", "current", "dam break|dam fail", "downburst|microburst", "drought|dry|dri|below normal precip", "drowning", "dust", "erosion|erosin", "excessive precip|heavy precip|record precip", "fire", "flood|floood", "fog", "freez", "frost", "funnel cloud|funnel", "glaze", "hail", "heat|hot|high temp|record high", "high water|rising water", "high waves", "hurricane|typhoon", "hyperthermia", "hypothermia", "ice|icy", "landslide|land", "lightning|lighting|ligntning", "marine accident|marine mishap", "mixed precip", "mudslide|mud", "northern lights", "other", "rain|shower", "red flag criteria", "rockslide|rock", "rogue wave", "seas", "seiche", "sleet", "smoke", "snow", "stream|strm", "surf", "surge", "swell", "thunderstorm|tstm$", "tide", "tornado|torndao|gustnado", "tropical depression", "tropical storm", "tsunami", "volcanic", "vog", "wall cloud", "warm", "waterspout|water spout|wayterspout", "wet", "wind|wnd|tstm wind|tstmw$", "winter|wintry")
for (str in types) {
  mask <- grepl(paste("(?i)", str, sep=""), data$EVTYPE)
  category <- data[mask, ]
  data[mask, ]$included <- TRUE
  data[mask, ]$newType <- paste(data[mask, ]$newType, str, sep=",")
}
mean(data$included) #coverage of original rows
levels(factor(data[data$included==F, ]$EVTYPE)) #non-meaningful types, left out
length(data$included[data$included == FALSE])  #total rows left out
```

Check that it worked as intended:

```{r}
head(table(data$newType)) #first row's new type is "", for non-meaningful
```

## Question 1: Across the United States, which types of events are most harmful with respect to population health?

### Analysis

First, subset out only fields which are relevant.

```{r}
popData <- data[c("newType", "FATALITIES", "INJURIES")]
```

Next, calculate total and average fatalities for each newType. Use a for loop
to iterate through list of types, and grep for entries to include.

```{r cache=TRUE}
popDamage <- data.frame(type = character(), TOTAL_FATALITIES = integer(), TOTAL_INJURIES = integer(), AVERAGE_FATALITIES = double(), AVERAGE_INJURIES = double())
for (str in types) {
  mask <- grepl(str, popData$newType)
  category <- data[mask, ]
  popDamage[nrow(popDamage) + 1, ] = list(str, sum(category$FATALITIES), sum(category$INJURIES), mean(category$FATALITIES), mean(category$INJURIES))
}
```

### Results: main question

Check simple maxima for total and average of injuries and fatalities.

```{r}
popDamage[which(popDamage$TOTAL_FATALITIES==max(popDamage$TOTAL_FATALITIES)), ]$type #type with most total fatalities
max(popDamage$TOTAL_FATALITIES) #most total fatalities
popDamage[which(popDamage$TOTAL_INJURIES==max(popDamage$TOTAL_INJURIES)), ]$type #type with most total injuries
max(popDamage$TOTAL_INJURIES) #most total injuries
popDamage[which(popDamage$AVERAGE_FATALITIES==max(popDamage$AVERAGE_FATALITIES)), ]$type #type with most average fatalities per event
max(popDamage$AVERAGE_FATALITIES) #most average fatalities
popDamage[which(popDamage$AVERAGE_INJURIES==max(popDamage$AVERAGE_INJURIES)), ]$type #type with most average injuries per event
max(popDamage$AVERAGE_INJURIES) #most average injuries
```

### Results: further exploration

When plotting histograms of the totals and averages of fatalities and injuries,
it is clear that most types are low-impact, and a few high-impact types make up
the bulk of the fatalities and injuries recorded - similarly, most events are
not dangerous individually, and only a few dangerous events with much higher
per-event injury and fatality rates.

```{r}
par(mfrow=c(2,2))
hist(popDamage$TOTAL_FATALITIES, breaks=100, xlab="Total Fatalities", ylab="Frequency", main="Histogram of Total Fatalities")
hist(popDamage$TOTAL_INJURIES, breaks=100, xlab="Total Injuries", ylab="Frequency", main="Histogram of Total Injuries")
hist(popDamage$AVERAGE_FATALITIES, breaks=100, xlab="Average Fatalities", ylab="Frequency", main="Histogram of Average Fatalities")
hist(popDamage$AVERAGE_INJURIES, breaks=100, xlab="Average Injuries", ylab="Frequency", main="Histogram of Average Injuries")
```

## Question 2: Across the United States, which types of events have the greatest economic consequences?

### Analysis

First, subset out only fields which are relevant.

```{r}
econData <- data[c("newType", "PROPDMG", "PROPDMGEXP")]
```

Since the coefficient and the exponent of property damage are stored separately
for each entry, use both to calculate the true numeric property damage. 
Calculated assuming exponents B=10^9, M=10^6, K=10^3, H=10^2. PROPDMGEXP values
"+", "-", and "?" were set to NA.

```{r cache=TRUE}
calcPropDmg <- function(row) {  #define function to apply to each row
  exp <- row[3]
  val <- as.numeric(row[2])
  if (exp %in% c(0,1,2,3,4,5,6,7,8)) {
    return(val * 10^as.numeric(exp))
  } else if (exp == 'B'){
    return(val * 10^9)
  } else if (exp == 'M' | exp == 'm') {
    return(val * 10^6)
  } else if (exp == 'K') {
    return(val * 10^3)
  } else if (exp == 'H' | exp == 'h') {
    return(val * 10^2)
  } else {
    return(NA)
  }
}
econData$realDamage <- apply(econData, MARGIN=1, FUN=calcPropDmg) #apply function
```

Next, calculate total and average fatalities for each newType. Use a for loop
to iterate through list of types, and grep for entries to include.

```{r cache=TRUE}
econDamage <- data.frame(type = character(), TOTAL_DAMAGE = integer(), AVERAGE_DAMAGE = double())
for (str in types) {
  mask <- grepl(str, econData$newType)
  category <- econData[mask, ]$realDamage
  econDamage[nrow(econDamage) + 1, ] = list(str, sum(category, na.rm=T), mean(category, na.rm=T))
}
```

### Results: main question

Check simple maxima for total and average of property damage

```{r}
econDamage[which(econDamage$TOTAL_DAMAGE == max(econDamage$TOTAL_DAMAGE, na.rm = T)), "type" ] #type with most total economic (property) damage
max(econDamage$TOTAL_DAMAGE, na.rm = T) #amount of total economic damage
econDamage[which(econDamage$AVERAGE_DAMAGE == max(econDamage$AVERAGE_DAMAGE, na.rm = T)), "type" ] #type with most average economic damage per event
max(econDamage$AVERAGE_DAMAGE, na.rm = T) #average economic damage
```

### Results: further exploration

Like with human damage, we again see sharp drops frequency with increasing total
and average economic damage. A few types of events are responsible for the bulk
of all economic damage from these disasters, and a few types of events are by
far the most damaging on a per-event basis. In this case, the drop appears even
sharper in terms of average damage than absolute damage. Interesting, on a
semi-log histogram (x=log(intensity), y=frequency), total damage seems about
evenly distributed, while average damage shows a significant clustering, with a
median of `r median(log10(econDamage$AVERAGE_DAMAGE), na.rm=T)` on a log scale,
or `r median(econDamage$AVERAGE_DAMAGE, na.rm=T)` dollars per event.

```{r}
par(mfrow=c(2,2))
hist(econDamage$TOTAL_DAMAGE, breaks=100, xlab="Total Damage ($)", ylab="Frequency", main="Histogram of Total Damage")
hist(econDamage$AVERAGE_DAMAGE, breaks=100, xlab="Average Damage ($)", ylab="Frequency", main="Histogram of Average Damage")
hist(log10(econDamage$TOTAL_DAMAGE), breaks=10, xlab="Log10 of Total Damage ($)", ylab="Frequency", main="Histogram of Log10 Total Damage")
hist(log10(econDamage$AVERAGE_DAMAGE), breaks=10, xlab="Log10 of Average Damage ($)", ylab="Frequency", main="Histogram of Log10 Average Damage")
abline(v=median(log10(econDamage$AVERAGE_DAMAGE), na.rm=T), col="red")
```